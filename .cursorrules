# .cursorrules - Context for Andy Warhol 3D Journal Visualization Project (Updated)

documentation:
  - project_name: Andy Warhol 3D Journal Visualization
  - description: |
      A WebXR application visualizing Andy Warhol's journal entries (~2000 from "The Andy Warhol Diaries")
      as an immersive 3D mindmap viewable on Oculus Quest 3. Uses NLP (OpenAI APIs) to analyze
      entries for emotional content, topics, and entities, creating an interactive landscape based
      on embeddings and UMAP dimensionality reduction. The audio system is being rebuilt using SuperCollider and OSC.
  - data_source: "The Andy Warhol Diaries" PDF
  - core_concept: 3D mindmap/landscape based on NLP analysis (emotion, topics, entities, embeddings) and UMAP dimensionality reduction. Dynamic audio generation via SuperCollider/OSC.
  - target_platform: WebXR on Oculus Quest 3 (with desktop fallback controls).
  - visualization_style: Abstract space environment with orbs representing entries, colored/sized by emotion, positioned by UMAP.

goals:
  - Process and analyze ~2000 journal entries using Python and OpenAI APIs (Completed).
  - Generate structured JSON data including text, date, location, sentiment, topics, entities, embeddings, UMAP coordinates, and related entries (Completed). Create an optimized version without embeddings for deployment (Completed).
  - Develop a Three.js WebXR application using Vite (Core structure completed).
  - Implement interactive exploration: movement (VR/desktop), orb selection, entry display panel, related entry highlighting (Completed).
  - Implement performance optimizations (LOD, culling) and monitoring tools (Completed).
  - **Replace** the previous Web Audio API file-based system with a dynamic SuperCollider-based audio system using OSC communication (Current Focus - Phase 5).
  - Refine the application, prepare for demonstration (local network setup, casting), and deploy via GitHub Pages.

project_status:
  - phase_1_data_processing: Completed
  - phase_2_basic_webxr_env: Completed (Including desktop controls)
  - phase_3_visualization_features: Completed
  - phase_4_1_performance_opts: Completed
  - phase_4_4_bug_fixes_refinements: Completed (Including JSON optimization script)
  - phase_4_2_dev_optimization_demo: In Progress / Next
  - phase_4_3_testing_presentation_setup: In Progress / Next
  - phase_5_supercollider_audio: Current Major Focus

dependencies:
  # Python Data Processing Dependencies (Phase 1 - Completed)
  python:
    - language_version: 3.x
    - main_libraries:
        - pdfplumber: For PDF text extraction.
        - openai: For interacting with GPT-4o mini and text-embedding-3-large.
        - umap-learn: For UMAP dimensionality reduction.
        - regex (re): For parsing journal entry structures.
        - numpy: For embedding/UMAP operations.
        - tqdm: For progress bars during batch processing.
    - utility_scripts:
        - `optimize_json.py`: Removes 'embedding' field from the final JSON for deployment.

  # JavaScript Visualization Application Dependencies
  javascript:
    - framework: three.js (Utilizing WebXR API, VRButton, XRControllerModelFactory).
    - build_tool: vite
    - key_libraries:
        - osc.js: For OSC communication with SuperCollider (Phase 5).
    - potential_libraries:
        - Text rendering library: e.g., troika-three-text or three-mesh-ui for VR text panels.
    - browser_apis:
        - WebXR API
        - Web Audio API (Potentially minimal use by osc.js, or for fallback if SC fails. Primary audio logic shifted to SuperCollider).

  # Audio Generation Dependencies (Phase 5 - Current Focus)
  supercollider:
    - language: SuperCollider (sclang)
    - core_concepts: SynthDefs, OSC communication (OSCdef), Patterns (Pbind).
    - main_file: `warholEmotions.scd` (likely location for SynthDefs and OSC setup).

  # Deployment
  deployment:
    - platform: GitHub Pages

technical_details:
  # Data Processing Pipeline (Python - Phase 1 - Completed)
  data_processing:
    - status: Completed
    - pipeline_summary: Extracted PDF -> Parsed Entries -> Grouped by Year -> Batched Analysis (GPT-4o mini: emotions, topics, entities) -> Batched Embeddings (text-embedding-3-large) -> Saved Interim -> Merged -> UMAP (n=3) -> Calculated Relations -> Saved Full JSON (`public/data/warhol_complete.json`).
    - final_optimization: Ran `optimize_json.py` script to remove `embedding` field, creating a smaller JSON (e.g., `warhol_optimized.json`) for application loading. The application likely loads this optimized version.

  # Visualization Application (Three.js / WebXR - Phases 2 & 3 - Completed)
  visualization:
    - status: Core features completed.
    - scene_setup: Basic Three.js scene, perspective camera, WebGLRenderer (XR enabled), dark background, lighting.
    - orbs: Spheres representing entries. Positioned by UMAP `coordinates`. Color based on dominant emotion (mapped from Plutchik). Size based on emotional intensity. UserData stores `entry` info.
    - connections: Lines drawn between selected orb and its `relatedEntries`.
    - text_panel: Floating panel showing selected entry details (date, text, topics, entities). Uses VR-friendly text rendering. Follows user gaze/position.

  # Interaction (WebXR / Three.js - Phase 2 & 3 - Completed)
  interaction:
    - status: Completed.
    - controllers_vr: Standard Quest 3 controllers via `XRControllerModelFactory`. Raycasting for selection.
    - movement_vr: Continuous thumbstick movement, optional teleportation.
    - controllers_desktop: Active when not in VR. WASD/Arrows (move), Mouse Drag (rotate), Space (up), Shift (down). Raycasting likely uses mouse position.
    - selection: Raycaster intersects interactive orbs. Highlights selected orb. Displays details in panel. Triggers related entry highlighting and audio (via OSC).
    - ui: Minimalist, VR-optimized controls (e.g., panel close button).

  # Audio (Phase 5 - Current Focus - Replacing Phase 3.5)
  audio:
    - status: Refactoring to SuperCollider + OSC (Phase 5). Previous Web Audio file-based system (Phase 3.5) is being replaced.
    - architecture: JavaScript (Three.js app) <-> OSC <-> SuperCollider Server.
    - supercollider_side (`warholEmotions.scd`):
        - Runs an OSC server (listening on e.g., port 57121).
        - Defines 8 `SynthDefs`, one for each Plutchik emotion, with unique characteristics.
        - Uses `OSCdef` to receive messages from the web app (e.g., `/warhol/emotion/joy value`).
        - Uses `Pbind` or other Pattern classes for dynamic, evolving sound generation based on received emotion values.
        - May include master effects (reverb, compression).
    - javascript_side:
        - `OscBridge.js`: Class handling OSC client setup and message sending using `osc.js`.
        - `AudioSystem.js`: Refactored to use `OscBridge.js`. Calculates emotion values based on proximity or selection. Sends OSC messages instead of playing local files. Manages mute state ('I' key toggle). Contains fallback logic if SC connection fails.
        - `playEntryAudio(entry)`: Function likely triggers sending the selected entry's full emotion profile via OSC.
    - deprecation: Code related to loading/playing `.mp3` files from `/public/sounds/` via Web Audio API is likely commented out or removed.

  # Performance Optimizations & Tools (Phase 4.1 - Completed)
  performance_optimizations:
    - status: Completed.
    - implementation: `PerformanceOptimizer` class handling frustum culling and Level of Detail (LOD - simpler geometry/materials for distant orbs). Smart geometry caching. Optimized update loops. Resource disposal functions.
    - monitoring: `PerformanceMonitor` class to display metrics.
    - known_fixes: Resolved issue where selected orbs froze when optimizations were active.
    - keyboard_toggles:
        - 'E': Toggle Emotion Wheel (Assumed UI element, verify implementation).
        - 'P': Toggle performance optimizations (on/off).
        - 'F': Toggle performance monitor display (on/off).
        - 'I': Toggle audio mute (on/off).

  # Demonstration Setup (Phase 4.2 / 4.3 - In Progress / Next)
  demonstration_setup:
    - focus: Preparing for presentation.
    - tasks: Configure Vite for local network HTTPS. Optimize data loading for network. Test Quest 3 casting. Projector setup. Create demo scripts/user guide.

# Data Structure Reference (Output from Python, Input to JavaScript)
data_structure:
  description: |
    The structure of the JSON data file generated by the Python script.
    Two versions exist:
    1. `warhol_complete.json`: Contains all fields, including the large `embedding` vector.
    2. `warhol_optimized.json` (or similar): Identical structure BUT *without* the `embedding` field. This smaller version is likely loaded by the WebXR application.
  format_example (Reflects structure before optimization): |
    {
      "entries": [
        {
          "id": "string",
          "date": "YYYY-MM-DD",
          "location": "string | null",
          "text": "string",
          "emotions": { // Plutchik's 8 emotions, scores 0.0-1.0
            "joy": 0.7, "trust": 0.5, "fear": 0.1, "surprise": 0.2,
            "sadness": 0.0, "disgust": 0.0, "anger": 0.0, "anticipation": 0.3
          },
          "topics": ["string", ...],
          "entities": { "people": ["string", ...], "places": ["string", ...] },
          "embedding": [float, float, ...], // *REMOVED in optimized version*
          "coordinates": {"x": float, "y": float, "z": float}, // UMAP 3D coordinates
          "relatedEntries": ["string", ...] // List of IDs of related entries
        }
        // ... ~2000 entries
      ]
    }